{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d119fdea-efb0-4e9f-a845-3678786e1815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78376141-4616-4179-86ae-7d30d26dc013",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f6b831-bc11-41b0-9307-1c4f5e6c5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.basic_features import *\n",
    "from utils import compute_intra_song_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f349588a-4507-4461-95ed-0b7972da37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = '../data/fma_small/000/000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b90090c-de60-40a1-84ab-f8d92ec4fa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files are:  ../data/fma_small/000/000002.mp3  and  ../data/fma_small/000/000368.mp3\n"
     ]
    }
   ],
   "source": [
    "track_id1 = \"002.mp3\"\n",
    "track_id2 = \"368.mp3\"\n",
    "filename1 = os.path.join(audio_path+track_id1)\n",
    "filename2 =  os.path.join(audio_path+track_id2)\n",
    "print('files are: ', filename1,' and ', filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b8dd55c-7e53-46cd-9755-6f8bb0e57b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_y, user_sr = librosa.load(filename1, sr=None)  # Load with native sample rate\n",
    "app_y, app_sr = librosa.load(filename2, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e434d76-c7bf-4574-9fcf-917623396fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44100, 44100, (1321967,), (1321967,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_sr, app_sr, user_y.shape, app_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe018b8-06c0-4b2e-8151-31347e1e59be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d465cd93-5e27-4abb-a571-cbf89b26534b",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17fbdf9c-41e1-4dd4-b527-5a1a13e27c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.24219862766167"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_feat_sim(filename1, filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "097d8241-f949-482b-b233-ef9b5a02bc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ml/51fw2kv13cd1_l56_3f1p0ww0000gn/T/ipykernel_3836/1778102124.py\", line 1, in <module>\n",
      "    compute_intra_song_similarity(audio_file=filename1, similarity_func=basic_feat_sim)\n",
      "  File \"/Users/atila/Documents/python/git/music/music/notebooks/utils.py\", line 30, in compute_intra_song_similarity\n",
      "    similarity = similarity_func(segment1, sr, segment2, sr)\n",
      "TypeError: basic_feat_sim() takes 2 positional arguments but 4 were given\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "compute_intra_song_similarity(audio_file=filename1, similarity_func=basic_feat_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5aa4de-f25a-47e5-b16d-955d58fafecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfafd208-a8f3-4d12-a08c-e3a9a1be4bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac2bff-23f6-4f74-aaae-a41e0b01f5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6c822-3012-49ae-8836-c4c8223bcc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e409460-1490-4766-afb1-a76fc7384b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ca216-f36f-43ac-bb2c-cd76998855f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840799fb-cb9f-41e0-b362-7f5f569a99e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2210b43f-0a7a-4b33-9e2f-a8657db6f882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b33d1-d697-4565-9abc-f3d648c23acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ae0a3-e8b7-4fec-919f-e0d025aa4906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec72d3-d9f5-49c3-afc0-b135bf394245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a9f95-8b20-4989-a999-606b592b3854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e86ca7-1984-40df-9ba6-ae7edd9ff3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07745db5-2c80-4e15-91f7-eec0535641db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f253f-310a-4af3-91e8-ecb448ed4492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27944eb8-302d-40da-9d1c-fde756f9dc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77814b17-727b-45c5-8f56-417edcef18a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5659b2c-ec95-443a-96b5-b9679452a346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d362b-60b6-4c55-88c0-895a0931682d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f7bbdce-d47f-4ed8-b1a3-d67ce8e6aa9f",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6af9db4b-2579-4e76-8ae6-65eb3a107c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " user_y =  (1321967,)\n",
      " app_y =  (1321967,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_y, user_sr = librosa.load(filename2, sr=None)  # Load with native sample rate\n",
    "app_y, app_sr = librosa.load(filename2, sr=None)\n",
    "\n",
    "print(\" user_y = \", user_y.shape)\n",
    "print(\" app_y = \", app_y.shape)\n",
    "print(\"\")\n",
    "\n",
    "# Ensure both audio clips have the same sample rate\n",
    "target_sample_rate = 44100\n",
    "if user_sr != target_sample_rate:\n",
    "    user_y = librosa.resample(user_y, user_sr, target_sample_rate)\n",
    "if app_sr != target_sample_rate:\n",
    "    app_y = librosa.resample(app_y, app_sr, target_sample_rate)\n",
    "\n",
    "# Preprocess user's audio\n",
    "user_y = noise_reduction(user_y, target_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ec51c-60da-4abd-93cb-b4216cb1c1ea",
   "metadata": {},
   "source": [
    "## Pitches Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb5da716-18fa-451b-9d97-f2dc3ae6bea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2582, 2582)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_pitches = calculate_pitch(user_y, target_sample_rate)\n",
    "app_pitches = calculate_pitch(app_y, target_sample_rate)\n",
    "len(user_pitches), len(app_pitches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4cbf426-c294-430e-a3fe-c8f1b0bb43cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.18512780790085"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = score_accuracy(user_pitches, app_pitches)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5542f7d0-f264-4593-b5a7-04d0040a5e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = score_accuracy(user_pitches, user_pitches)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57ee83-76ce-451e-bbee-31db62785bc0",
   "metadata": {},
   "source": [
    "## Timing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1413d-949a-4530-9d63-82de657d5274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee6cc81-7984-44c5-ae63-54f023a4390d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 122)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_onsets = calculate_onset_times(user_y, target_sample_rate)\n",
    "app_onsets = calculate_onset_times(app_y, target_sample_rate)\n",
    "len(user_onsets), len(app_onsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21e4bcac-5f4e-4fe1-ab2c-1406e959342e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing = score_timing(user_onsets, app_onsets)\n",
    "timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7345df2-2b84-4796-9693-90fea58c9c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   3,   25,   52,  278,  314,  385,  407,  425,  429,  464,  540,\n",
       "         560,  599,  617,  620,  656,  679,  770,  829,  851,  880,  899,\n",
       "         914,  926,  943,  985, 1026, 1048, 1070, 1114, 1129, 1141, 1152,\n",
       "        1215, 1232, 1235, 1308, 1317, 1334, 1344, 1378, 1405, 1437, 1535,\n",
       "        1606, 1623, 1655, 1687, 1827, 1845, 1848, 1863, 1903, 1937, 1973,\n",
       "        2069, 2108, 2146, 2187, 2204, 2225, 2244, 2299, 2345, 2376, 2415,\n",
       "        2435, 2448, 2453, 2463]),\n",
       " 70)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onset_frames = librosa.onset.onset_detect(y=user_y, sr=target_sample_rate)\n",
    "onset_frames, len(onset_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4dc3d33-6413-4046-b561-f15aa02a627e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03482993,  0.29024943,  0.60371882,  3.2275737 ,  3.64553288,\n",
       "         4.46984127,  4.72526077,  4.93424036,  4.98068027,  5.38702948,\n",
       "         6.26938776,  6.5015873 ,  6.95437642,  7.16335601,  7.19818594,\n",
       "         7.61614512,  7.8831746 ,  8.93968254,  9.6246712 ,  9.8800907 ,\n",
       "        10.21678005, 10.43736961, 10.61151927, 10.750839  , 10.94820862,\n",
       "        11.43582766, 11.91183673, 12.16725624, 12.42267574, 12.93351474,\n",
       "        13.1076644 , 13.24698413, 13.37469388, 14.10612245, 14.30349206,\n",
       "        14.338322  , 15.18585034, 15.29034014, 15.48770975, 15.60380952,\n",
       "        15.99854875, 16.31201814, 16.68353741, 17.82131519, 18.64562358,\n",
       "        18.8429932 , 19.21451247, 19.58603175, 21.21142857, 21.42040816,\n",
       "        21.4552381 , 21.62938776, 22.09378685, 22.48852608, 22.90648526,\n",
       "        24.02104308, 24.4738322 , 24.91501134, 25.39102041, 25.58839002,\n",
       "        25.83219955, 26.05278912, 26.69133787, 27.22539683, 27.58530612,\n",
       "        28.03809524, 28.27029478, 28.42122449, 28.47927438, 28.59537415]),\n",
       " 70)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onset_times = librosa.frames_to_time(onset_frames, sr=target_sample_rate)\n",
    "onset_times, len(onset_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ddeae19-ddea-463d-8bfb-6d5b887b5442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 122)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_onsets = calculate_onset_times(user_y, target_sample_rate)\n",
    "# We don't adjust onset times for the app's audio as it starts from t=0.\n",
    "app_onsets = librosa.frames_to_time(librosa.onset.onset_detect(y=app_y, sr=target_sample_rate), sr=target_sample_rate)\n",
    "len(user_onsets), len(app_onsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c400344a-d883-495f-8a0d-ef70719862eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing = score_timing(user_onsets, app_onsets)\n",
    "timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8abffcf2-6f99-4698-9248-95efea15ecfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing = score_timing(user_onsets, user_onsets)\n",
    "timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a18cfb-fb0f-46fe-8d87-183302477219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca791b40-73b4-4b70-aebc-48de4d8f1654",
   "metadata": {},
   "source": [
    "## Fluency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107de20-7b07-4908-be2c-1eb8d98e5163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45f707c2-ad39-4602-b637-fbfbcedf8d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_onsets = calculate_onset_times(user_y, target_sample_rate)\n",
    "len(user_onsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1531b922-6f90-49b2-b3f9-93dcd5e6702d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-250"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fluency = 100 - len(calculate_onset_times(user_y, target_sample_rate)) * 5\n",
    "fluency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f5a782-f7a0-4515-b49f-a5a97469b3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65ccba40-9b16-4139-8686-7aa6315b6c48",
   "metadata": {},
   "source": [
    "## All of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "204c7727-ea19-423e-b21a-be6c76684593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files are:  ../data/fma_small/000/000002.mp3  and  ../data/fma_small/000/000368.mp3\n"
     ]
    }
   ],
   "source": [
    "track_id1 = \"002.mp3\"\n",
    "track_id2 = \"368.mp3\"\n",
    "filename1 = os.path.join(audio_path+track_id1)\n",
    "filename2 =  os.path.join(audio_path+track_id2)\n",
    "print('files are: ', filename1,' and ', filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb5b5084-272c-41be-9720-477681379b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.24219862766167"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_feat_sim(filename1, filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a1137-cf69-46d8-b931-8d098d92370e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2692834-a26b-47f4-b5f3-224281e73798",
   "metadata": {},
   "source": [
    "# Cosine Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9722d2d-6566-4fa6-a443-5432a6a7eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52224161-0683-4f2f-a6c1-9e03c8393b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41364145"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([user_y], [app_y])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af76b1-ca8b-4e55-80eb-957be6029987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ecacf9b-6bcc-4711-9b1e-d843041bac9f",
   "metadata": {},
   "source": [
    "# Testing with common parts of the same song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4620a280-6e17-4577-bda8-44675af56203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import soundfile as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6746b468-8c3d-429a-9b3a-fe861b1c7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intra_song_similarity(audio_path: str, similarity_func: Callable=basic_feat_sim) -> float:\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "    # Calculate the one-third and two-thirds points of the audio\n",
    "    one_third_point = int(1/3 * len(y))\n",
    "    two_third_point = int(2/3 * len(y))\n",
    "\n",
    "    # Split the audio into the two parts based on these points\n",
    "    part1 = y[:two_third_point]\n",
    "    part2 = y[one_third_point:]\n",
    "\n",
    "    # Save each part as a temporary WAV file\n",
    "    sf.write('temp_part1.wav', part1, sr)\n",
    "    sf.write('temp_part2.wav', part2, sr)\n",
    "    \n",
    "    # Use the passed similarity function to compute the score\n",
    "    similarity_score = similarity_func('temp_part1.wav', 'temp_part2.wav')\n",
    "    \n",
    "    # Cleanup temporary files (optional)\n",
    "    import os\n",
    "    os.remove('temp_part1.wav')\n",
    "    os.remove('temp_part2.wav')\n",
    "\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94c7c213-d96e-45ee-a017-e4f0f76af193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intra-song similarity score: 22.89%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test\n",
    "audio_path = filename1\n",
    "score = compute_intra_song_similarity(audio_path)\n",
    "print(f\"Intra-song similarity score: {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7ad045b-2839-4c28-9505-efea3e6a8e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intra-song similarity score: 18.99%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test\n",
    "audio_path = filename2\n",
    "score = compute_intra_song_similarity(audio_path)\n",
    "print(f\"Intra-song similarity score: {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10bf35b8-0d93-400b-80b2-6781cae4eb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " score: 20.24%\n"
     ]
    }
   ],
   "source": [
    "score = basic_feat_sim(filename1, filename2)\n",
    "print(f\" score: {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bfeef5b-08d7-42d8-b02e-e33bdb25b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files are:  ../data/fma_small/000/000002.mp3  and  ../data/fma_small/000/000005.mp3 \n",
      "\n",
      "Intra-song similarity score for song ../data/fma_small/000/000002.mp3 is: 22.89%\n",
      "Intra-song similarity score for song ../data/fma_small/000/000005.mp3 is: 32.99%\n",
      "between songs score is: 28.00%\n"
     ]
    }
   ],
   "source": [
    "audio_path = '../data/fma_small/000/000'\n",
    "\n",
    "track_id1 = \"002.mp3\"\n",
    "track_id2 = \"005.mp3\"\n",
    "filename1 = os.path.join(audio_path+track_id1)\n",
    "filename2 =  os.path.join(audio_path+track_id2)\n",
    "print('files are: ', filename1,' and ', filename2, '\\n')\n",
    "\n",
    "audio_path = filename1\n",
    "score = compute_intra_song_similarity(audio_path)\n",
    "print(f\"Intra-song similarity score for song {audio_path} is: {score:.2f}%\")\n",
    "\n",
    "audio_path = filename2\n",
    "score = compute_intra_song_similarity(audio_path)\n",
    "print(f\"Intra-song similarity score for song {audio_path} is: {score:.2f}%\")\n",
    "\n",
    "score = basic_feat_sim(filename1, filename2)\n",
    "print(f\"between songs score is: {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a00bc-ea19-4205-a600-a5b212a93334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de405d28-ec37-4274-a83a-39e5eedfaebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748efd1b-6b37-4ea4-adf2-4ff7d6b5360d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad31de2-5d4b-49e8-bdb2-f0106e9f875f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce831468-f98e-4788-8a4b-9d0e997a5893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "192df79a-2711-4488-8224-3fa50697770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_intra_song_similarity(audio_path: str, similarity_func: Callable=basic_feat_sim) -> float:\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "    # Calculate the one-third and two-thirds points of the audio\n",
    "    one_third_point = int(1/3 * len(y))\n",
    "    two_third_point = int(2/3 * len(y))\n",
    "\n",
    "    # Split the audio into the two parts based on these points\n",
    "    part1 = y[:two_third_point]\n",
    "    part2 = y[one_third_point:]\n",
    "\n",
    "    # Compute similarity score between the two parts\n",
    "    # Here, I'm using the previously defined 'evaluate_audio' function\n",
    "    # You might need to adjust this based on your preferred similarity metric\n",
    "    # For this to work, save each part as a temporary WAV file or adjust the evaluate_audio function to accept raw audio instead of paths\n",
    "\n",
    "    librosa.output.write_wav('temp_part1.wav', part1, sr)\n",
    "    librosa.output.write_wav('temp_part2.wav', part2, sr)\n",
    "    \n",
    "    similarity_score = similarity_func('temp_part1.wav', 'temp_part2.wav')\n",
    "    \n",
    "    # Cleanup temporary files (optional)\n",
    "    os.remove('temp_part1.wav')\n",
    "    os.remove('temp_part2.wav')\n",
    "\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8947a3ac-6482-4d6f-a507-3509bc0efeb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "No librosa attribute output",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[1;32m      2\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m filename1\n\u001b[0;32m----> 3\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_intra_song_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntra-song similarity score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 18\u001b[0m, in \u001b[0;36mcompute_intra_song_similarity\u001b[0;34m(audio_path, similarity_func)\u001b[0m\n\u001b[1;32m     11\u001b[0m part2 \u001b[38;5;241m=\u001b[39m y[one_third_point:]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Compute similarity score between the two parts\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Here, I'm using the previously defined 'evaluate_audio' function\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# You might need to adjust this based on your preferred similarity metric\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# For this to work, save each part as a temporary WAV file or adjust the evaluate_audio function to accept raw audio instead of paths\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mwrite_wav(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_part1.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, part1, sr)\n\u001b[1;32m     19\u001b[0m librosa\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mwrite_wav(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_part2.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, part2, sr)\n\u001b[1;32m     21\u001b[0m similarity_score \u001b[38;5;241m=\u001b[39m similarity_func(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_part1.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_part2.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/audio/lib/python3.9/site-packages/lazy_loader/__init__.py:89\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attr\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: No librosa attribute output"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test\n",
    "audio_path = filename1\n",
    "score = compute_intra_song_similarity(audio_path)\n",
    "print(f\"Intra-song similarity score: {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01717598-4ede-47d4-a19d-669ff057a64e",
   "metadata": {},
   "source": [
    "# Other Futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18f56a-3692-4f80-bcf1-ae5ed1059867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a2fe57f-13aa-4d49-aef3-9b187b81c981",
   "metadata": {},
   "source": [
    "## Spectral Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d421026a-66e1-4184-8195-8a90b4c54221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectral_centroid(y: np.ndarray, sr: int) -> float:\n",
    "    \"\"\"\n",
    "    Extracts the mean spectral centroid from the audio data.\n",
    "    \n",
    "    :param y: Audio time series.\n",
    "    :param sr: Sampling rate.\n",
    "    :return: Mean spectral centroid.\n",
    "    \"\"\"\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    return float(np.mean(spectral_centroid))\n",
    "\n",
    "def compute_spectral_centroid_similarity(user_y: np.ndarray, user_sr: int, \n",
    "                                         app_y: np.ndarray, app_sr: int) -> float:\n",
    "    \"\"\"\n",
    "    Computes the similarity score based on spectral centroid between user's audio and the reference.\n",
    "    \n",
    "    :param user_y: User's audio time series.\n",
    "    :param user_sr: User's audio sampling rate.\n",
    "    :param app_y: Reference audio time series.\n",
    "    :param app_sr: Reference audio sampling rate.\n",
    "    :return: Similarity score (ranging from 0 to 1).\n",
    "    \"\"\"\n",
    "    # Extracting spectral centroid\n",
    "    user_centroid = extract_spectral_centroid(user_y, user_sr)\n",
    "    app_centroid = extract_spectral_centroid(app_y, app_sr)\n",
    "\n",
    "    # Computing the absolute difference between the centroids\n",
    "    diff = abs(user_centroid - app_centroid)\n",
    "    \n",
    "    # Convert difference to similarity (this is a simplistic transformation, and you can use a more complex one if needed)\n",
    "    max_possible_diff = max(user_centroid, app_centroid)  # This assumes one of the centroids is the highest possible value\n",
    "    similarity = 1 - (diff / max_possible_diff)\n",
    "    \n",
    "    return float(similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2baff2ec-7d3c-45f0-a8fa-4e66411e02a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files are:  ../data/fma_small/000/000002.mp3  and  ../data/fma_small/000/000005.mp3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "audio_path = '../data/fma_small/000/000'\n",
    "\n",
    "track_id1 = \"002.mp3\"\n",
    "track_id2 = \"005.mp3\"\n",
    "filename1 = os.path.join(audio_path+track_id1)\n",
    "filename2 =  os.path.join(audio_path+track_id2)\n",
    "print('files are: ', filename1,' and ', filename2, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c57c2bd-a785-41a5-8ec0-bd997ee9467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_y, user_sr = librosa.load(filename1, sr=None)\n",
    "app_y, app_sr = librosa.load(filename2, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "513e80b4-d006-4859-be0a-0566743ca940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7948970309016195"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_spectral_centroid_similarity(user_y, user_sr, app_y, app_sr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37bc8228-d7e8-42c9-89d7-76ce2a5e6bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3683.997535313735"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_centroid = extract_spectral_centroid(user_y, user_sr)\n",
    "user_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12636e84-1f7a-41c9-939c-eb5260ecf41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2928.398702669772"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_centroid = extract_spectral_centroid(app_y, app_sr)\n",
    "app_centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3546dcc-cd08-45f3-b2f6-9872d50a4c92",
   "metadata": {},
   "source": [
    "## Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2cb657c3-efa5-448f-baba-ad35bb06b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "# import librosa.display\n",
    "\n",
    "def compute_spectrogram(y: np.ndarray, sr: int) -> np.ndarray:\n",
    "    \"\"\"Compute the magnitude spectrogram of an audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "        y (np.ndarray): The audio time series.\n",
    "        sr (int): The sample rate of the audio time series.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The magnitude spectrogram.\n",
    "    \"\"\"\n",
    "    # Compute the short-time Fourier transform (STFT) of the audio\n",
    "    D = np.abs(librosa.stft(y))\n",
    "    # Convert the STFT to a magnitude spectrogram\n",
    "    return librosa.amplitude_to_db(D, ref=np.max)\n",
    "\n",
    "\n",
    "\n",
    "def spectrogram_similarity(spectrogram1: np.ndarray, spectrogram2: np.ndarray) -> float:\n",
    "    \"\"\"Compute similarity between two spectrograms using MSE.\n",
    "    \n",
    "    Parameters:\n",
    "        spectrogram1 (np.ndarray): The magnitude spectrogram of the first audio.\n",
    "        spectrogram2 (np.ndarray): The magnitude spectrogram of the second audio.\n",
    "        \n",
    "    Returns:\n",
    "        float: The mean squared error between the two spectrograms.\n",
    "    \"\"\"\n",
    "    # Ensure both spectrograms are of the same shape by trimming or padding\n",
    "    min_time = min(spectrogram1.shape[1], spectrogram2.shape[1])\n",
    "    spectrogram1 = spectrogram1[:, :min_time]\n",
    "    spectrogram2 = spectrogram2[:, :min_time]\n",
    "\n",
    "    mse = np.mean((spectrogram1 - spectrogram2)**2)\n",
    "    print(\"mse = \", mse)\n",
    "    \n",
    "    # For similarity, we'd often want a measure where higher values indicate more similarity.\n",
    "    # So, we can transform the MSE into a similarity measure by taking its inverse or negative.\n",
    "    similarity = 1 / (1 + np.exp(mse / -10.0))\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "\n",
    "def spectrogram_cosine_similarity(spectrogram1: np.ndarray, spectrogram2: np.ndarray) -> float:\n",
    "    \"\"\"Compute the cosine similarity between the spectrograms of two audio signals.\n",
    "    \n",
    "    Parameters:\n",
    "        y1 (np.ndarray): The first audio signal.\n",
    "        sr1 (int): The sample rate of the first audio signal.\n",
    "        y2 (np.ndarray): The second audio signal.\n",
    "        sr2 (int): The sample rate of the second audio signal.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity between the two spectrograms.\n",
    "    \"\"\"\n",
    "    # # Compute the spectrograms\n",
    "    # spec1 = compute_spectrogram(y1, sr1)\n",
    "    # spec2 = compute_spectrogram(y2, sr2)\n",
    "    \n",
    "    # Flatten the spectrogram matrices to make them vectors\n",
    "    spec1_vec = spectrogram1.flatten()\n",
    "    spec2_vec = spectrogram2.flatten()\n",
    "\n",
    "    # Ensure that both vectors have the same shape\n",
    "    min_len = min(len(spec1_vec), len(spec2_vec))\n",
    "    spec1_vec = spec1_vec[:min_len]\n",
    "    spec2_vec = spec2_vec[:min_len]\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(spec1_vec.reshape(1, -1), spec2_vec.reshape(1, -1))\n",
    "\n",
    "    return similarity[0][0]\n",
    "\n",
    "\n",
    "\n",
    "def compute_spectrogram_similarity(user_y: np.ndarray, user_sr: int, \n",
    "                                   app_y: np.ndarray, app_sr: int, \n",
    "                                   use_cosine: bool=True) -> float:\n",
    "\n",
    "    \"\"\"Compute similarity between two frames.\n",
    "    \n",
    "    Parameters:\n",
    "        :param user_y: User's audio time series.\n",
    "        :param user_sr: User's audio sampling rate.\n",
    "        :param app_y: Reference audio time series.\n",
    "        :param app_sr: Reference audio sampling rate.\n",
    "    Returns:\n",
    "        float: sim score.\n",
    "    \"\"\"\n",
    "\n",
    "    user_spec = compute_spectrogram(user_y, user_sr)\n",
    "    app_spec = compute_spectrogram(app_y, app_sr)\n",
    "\n",
    "    print('user_spec = ', user_spec.shape)\n",
    "\n",
    "    if use_cosine:\n",
    "        return spectrogram_cosine_similarity(user_spec, app_spec)\n",
    "\n",
    "    return spectrogram_similarity(user_spec, app_spec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a9b79695-ef52-4137-85e0-bee5e0e92cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_y, user_sr = librosa.load(filename1, sr=None)\n",
    "app_y, app_sr = librosa.load(filename2, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "de4e3d7f-371f-4717-9327-3d130a40fb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_spec =  (1025, 2582)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98262715"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_spectrogram_similarity(user_y, user_sr, app_y, app_sr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "70ee37a5-36c9-4731-97e4-a591ed1d0787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_spec =  (1025, 2582)\n",
      "mse =  154.94618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999998134596769"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_spectrogram_similarity(user_y, user_sr, app_y, app_sr, use_cosine=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce86a2-e82d-421c-91bb-1d47a478410f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983d24e-af4a-4c67-a0e4-c5bc12044396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f81c6-d253-4719-87ea-aaab659f27b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "61c766e2-358f-49f5-b25f-d1f4d31e91d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1025, 1025)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_spec = compute_spectrogram(user_y, user_sr)\n",
    "app_spec = compute_spectrogram(app_y, app_sr)\n",
    "len(user_spec), len(app_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca157d16-3ec3-4b57-afe9-67ee05716150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.059023205018258e-07"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981d101-6de3-4989-983b-4f1105324251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1004b78-6fae-427e-a9ca-b946f72458cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6cee8-aa4a-4cb7-8af1-ea55d31b36e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3040d304-a70e-4341-bed9-d57777b9178e",
   "metadata": {},
   "source": [
    "## Spectral Bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2abaf13b-0b58-43eb-a74a-a5b677670968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f31b302a-ad1c-4e81-ba02-3edf924be3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2582"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = librosa.feature.spectral_bandwidth(y=user_y, sr=user_sr)[0]\n",
    "len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e8a55226-5d01-4757-bc82-4f46c715b062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2585"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = librosa.feature.spectral_bandwidth(y=app_y, sr=app_sr)[0]\n",
    "len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "47bccc44-fafa-4553-9919-50a7af2ffe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_bandwidth(y: np.ndarray, sr: int) -> np.ndarray:\n",
    "    \"\"\"Compute the spectral bandwidth of an audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "        y (np.ndarray): The audio signal.\n",
    "        sr (int): The sample rate of the audio signal.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The computed spectral bandwidth.\n",
    "    \"\"\"\n",
    "    return librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
    "\n",
    "\n",
    "\n",
    "def resample_to_match(y1: np.ndarray, y2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Resample both signals to have the same length based on the shorter signal.\"\"\"\n",
    "    \n",
    "    target_length = min(len(y1), len(y2))\n",
    "    y1_resampled = y1[:target_length]\n",
    "    y2_resampled = y2[:target_length]\n",
    "    \n",
    "    return y1_resampled, y2_resampled\n",
    "\n",
    "\n",
    "def align_sequences(seq1: np.ndarray, seq2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if len(seq1) > len(seq2):\n",
    "        pad_length = len(seq1) - len(seq2)\n",
    "        seq2 = np.pad(seq2, (0, pad_length), mode='constant')\n",
    "    elif len(seq1) < len(seq2):\n",
    "        pad_length = len(seq2) - len(seq1)\n",
    "        seq1 = np.pad(seq1, (0, pad_length), mode='constant')\n",
    "    return seq1, seq2\n",
    "\n",
    "\n",
    "def spectral_bandwidth_similarity(y1: np.ndarray, sr1: int, y2: np.ndarray, sr2: int, use_padding: bool=True) -> float:\n",
    "    \"\"\"Compute the similarity between the spectral bandwidths of two audio signals.\n",
    "    \n",
    "    Parameters:\n",
    "        y1 (np.ndarray): The first audio signal.\n",
    "        sr1 (int): The sample rate of the first audio signal.\n",
    "        y2 (np.ndarray): The second audio signal.\n",
    "        sr2 (int): The sample rate of the second audio signal.\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity between the two spectral bandwidths.\n",
    "    \"\"\"\n",
    "    # Compute the spectral bandwidths\n",
    "    sb1 = compute_spectral_bandwidth(y1, sr1)\n",
    "    sb2 = compute_spectral_bandwidth(y2, sr2)\n",
    "\n",
    "    if len(sb1)!=len(sb2):\n",
    "        # Align the lengths of the two sequences\n",
    "        if use_padding:\n",
    "            sb1, sb2 = align_sequences(sb1, sb2)\n",
    "        else:\n",
    "            # Resample the audio clips to have the same length\n",
    "            y1, y2 = resample_to_match(y1, y2)\n",
    "            \n",
    "            # Compute the spectral bandwidth for both audios\n",
    "            sb1 = librosa.feature.spectral_bandwidth(y=y1, sr=sr1)\n",
    "            sb2 = librosa.feature.spectral_bandwidth(y=y2, sr=sr2)\n",
    "     \n",
    "    # Compute the mean absolute difference between the two spectral bandwidth sequences\n",
    "    # and normalize it to obtain a similarity score between 0 and 1\n",
    "    \n",
    "    # mean_difference = np.mean(np.abs(sb1 - sb2))\n",
    "    # max_possible_difference = np.max([np.max(sb1), np.max(sb2)]) - np.min([np.min(sb1), np.min(sb2)])\n",
    "    # similarity = 1 - (mean_difference / max_possible_difference)\n",
    "\n",
    "    similarity = 1 - np.mean(np.abs(sb1 - sb2) / (np.abs(sb1) + np.abs(sb2) + 1e-10))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c5a125cd-a314-49f8-a1ac-1e751aa21cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_y, user_sr = librosa.load(filename1, sr=None)\n",
    "app_y, app_sr = librosa.load(filename2, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c9a668cd-7b78-4980-943e-236409bcf40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8977201952301201"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_bandwidth_similarity(user_y, user_sr, app_y, app_sr, use_padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1a8809b2-19c6-4616-af3f-7e84b23767cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8966906733409049"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_bandwidth_similarity(user_y, user_sr, app_y, app_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b6e9be-1335-4c89-a180-f5ddccc42ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "832c0053-35c1-4199-b964-b8ec532d54ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8977201952301201"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_bandwidth_similarity(user_y, user_sr, app_y, app_sr, use_padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3d03e223-bb66-483d-8744-60d71312106a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8966906733409049"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_bandwidth_similarity(user_y, user_sr, app_y, app_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610d81c-9373-42d5-bf39-44880c0f370a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cd525d7-456b-4887-ad2b-988437327ca8",
   "metadata": {},
   "source": [
    "## Spectral Contras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "717f6fef-88b3-4248-8462-7ac73dc87d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_contrast_similarity(y1: np.ndarray, sr1: int, y2: np.ndarray, sr2: int, use_padding: bool=True) -> float:\n",
    "    # If use_padding is true, align sequences using zero-padding\n",
    "    if use_padding:\n",
    "        y1, y2 = align_sequences(y1, y2)\n",
    "    else:\n",
    "        # Resample the audio clips to have the same length\n",
    "        y1, y2 = resample_to_match(y1, sr1, y2, sr2)\n",
    "    \n",
    "    # Compute the spectral contrast for both audios\n",
    "    S1 = librosa.feature.spectral_contrast(y=y1, sr=sr1)\n",
    "    S2 = librosa.feature.spectral_contrast(y=y2, sr=sr2)\n",
    "    \n",
    "    # Compute similarity\n",
    "    mean_difference = np.mean(np.abs(S1 - S2))\n",
    "    max_possible_difference = np.max([np.max(S1), np.max(S2)]) - np.min([np.min(S1), np.min(S2)])\n",
    "    similarity = 1 - (mean_difference / max_possible_difference)\n",
    "\n",
    "    # similarity = 1 - np.mean(np.abs(S1 - S2) / (np.abs(S1) + np.abs(S2) + 1e-10))\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "\n",
    "def spectral_contrast_similarity(y1: np.ndarray, sr1: int, y2: np.ndarray, sr2: int, use_padding: bool = False) -> float:\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the similarity between two audio signals based on their spectral contrasts.\n",
    "\n",
    "    Parameters:\n",
    "    - y1 (np.ndarray): The waveform array for the first audio signal.\n",
    "    - sr1 (int): The sampling rate of the first audio signal.\n",
    "    - y2 (np.ndarray): The waveform array for the second audio signal.\n",
    "    - sr2 (int): The sampling rate of the second audio signal.\n",
    "    - use_padding (bool, optional): Whether to use padding or resampling when aligning audio lengths. Defaults to False (resampling).\n",
    "\n",
    "    Returns:\n",
    "    - float: The similarity score based on spectral contrast, where 1 represents maximum similarity and 0 represents minimum similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the spectral contrast for both audio signals\n",
    "    S1 = librosa.feature.spectral_contrast(y=y1, sr=sr1)\n",
    "    S2 = librosa.feature.spectral_contrast(y=y2, sr=sr2)\n",
    "    \n",
    "    # Align the computed spectral contrasts\n",
    "    if use_padding:\n",
    "        S1, S2 = align_sequences(S1, S2)\n",
    "    else:\n",
    "        y1, y2 = resample_to_match(y1, y2)\n",
    "        S1 = librosa.feature.spectral_contrast(y=y1, sr=sr1)\n",
    "        S2 = librosa.feature.spectral_contrast(y=y2, sr=sr2)\n",
    "    \n",
    "    # Compute the similarity measure\n",
    "    mean_difference = np.mean(np.abs(S1 - S2))\n",
    "    max_possible_difference = np.max([np.max(S1), np.max(S2)]) - np.min([np.min(S1), np.min(S2)])\n",
    "    similarity = 1 - (mean_difference / max_possible_difference)\n",
    "\n",
    "    # similarity = 1 - np.mean(np.abs(S1 - S2) / (np.abs(S1) + np.abs(S2) + 1e-10))\n",
    "    \n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "dadd1268-f51f-4248-974f-ee3d410f19ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9345887961784565"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_contrast_similarity(user_y, user_sr, app_y, app_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8892a28f-79c7-4a90-9f06-25ac783e36cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80066df9-eeb7-4754-be84-564d5193e20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ed99a-212e-4d8a-86c2-0ca786abf2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e2156-025c-409a-bcef-d7bc768222b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07031776-bf90-4d67-84da-15c682a8769a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369c4eb-796a-44e1-ab31-dc2e93b322be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6139a351-4c85-4767-98af-0493a98bc118",
   "metadata": {},
   "source": [
    "## Spectral Flatness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f9db83a5-c209-4d75-81b7-b8606a5f242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_flatness_similarity(y1: np.ndarray, sr1: int, y2: np.ndarray, sr2: int, use_padding: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Compute the similarity between two audio signals based on their spectral flatness.\n",
    "\n",
    "    Parameters:\n",
    "    - y1 (np.ndarray): The waveform array for the first audio signal.\n",
    "    - sr1 (int): The sampling rate of the first audio signal.\n",
    "    - y2 (np.ndarray): The waveform array for the second audio signal.\n",
    "    - sr2 (int): The sampling rate of the second audio signal.\n",
    "    - use_padding (bool, optional): Whether to use padding or resampling when aligning audio lengths. Defaults to False (resampling).\n",
    "\n",
    "    Returns:\n",
    "    - float: The similarity score based on spectral flatness, where 1 represents maximum similarity and 0 represents minimum similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the spectral flatness for both audio signals\n",
    "    SF1 = librosa.feature.spectral_flatness(y=y1)\n",
    "    SF2 = librosa.feature.spectral_flatness(y=y2)\n",
    "    \n",
    "    # Align the computed spectral flatness values\n",
    "    if use_padding:\n",
    "        SF1, SF2 = align_sequences(SF1, SF2)\n",
    "    else:\n",
    "        y1, y2 = resample_to_match(y1, y2)\n",
    "        SF1 = librosa.feature.spectral_flatness(y=y1)\n",
    "        SF2 = librosa.feature.spectral_flatness(y=y2)\n",
    "    \n",
    "    # Compute the similarity measure\n",
    "    mean_difference = np.mean(np.abs(SF1 - SF2))\n",
    "    max_possible_difference = np.max([np.max(SF1), np.max(SF2)]) - np.min([np.min(SF1), np.min(SF2)])\n",
    "    similarity = 1 - (mean_difference / max_possible_difference)\n",
    "    \n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d5fdcd87-924c-4004-9de1-370496d249f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9690905902534723"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_flatness_similarity(user_y, user_sr, app_y, app_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bfce4d-7d4b-4e89-b7f4-557cb161234d",
   "metadata": {},
   "source": [
    "### Aggregate Spectral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7193fa45-cc24-439d-83f4-0676872c0e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_spectral_similarity(user_y: np.ndarray, user_sr: int, app_y: np.ndarray, app_sr: int, \n",
    "                                  weights: dict = None) -> float:\n",
    "    \"\"\"\n",
    "    Compute an aggregate similarity score based on various spectral features.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_y: Audio time series for the user.\n",
    "    - user_sr: Sampling rate of user's audio.\n",
    "    - app_y: Audio time series for the app/reference.\n",
    "    - app_sr: Sampling rate of app's audio.\n",
    "    - weights: Dictionary containing weights for each feature. If None, default weights are used.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Aggregate similarity score between 0 (completely dissimilar) and 1 (identical).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default weights\n",
    "    if weights is None:\n",
    "        weights = {\n",
    "            'centroid': 0.2,\n",
    "            'spectrogram': 0.3,\n",
    "            'bandwidth': 0.2,\n",
    "            'contrast': 0.15,\n",
    "            'flatness': 0.15\n",
    "        }\n",
    "    \n",
    "    # Calculate similarities for each feature\n",
    "    centroid_sim = compute_spectral_centroid_similarity(user_y, user_sr, app_y, app_sr)\n",
    "    spectrogram_sim = compute_spectrogram_similarity(user_y, user_sr, app_y, app_sr)\n",
    "    bandwidth_sim = spectral_bandwidth_similarity(user_y, user_sr, app_y, app_sr)\n",
    "    contrast_sim = spectral_contrast_similarity(user_y, user_sr, app_y, app_sr)\n",
    "    flatness_sim = spectral_flatness_similarity(user_y, user_sr, app_y, app_sr)\n",
    "    \n",
    "    # Aggregate with weights\n",
    "    aggregate_similarity = (\n",
    "        weights['centroid'] * centroid_sim +\n",
    "        weights['spectrogram'] * spectrogram_sim +\n",
    "        weights['bandwidth'] * bandwidth_sim +\n",
    "        weights['contrast'] * contrast_sim +\n",
    "        weights['flatness'] * flatness_sim\n",
    "    )\n",
    "    \n",
    "    return aggregate_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c2b06fef-cb54-4841-88df-929a40804724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_spec =  (1025, 2582)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9186575948322762"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_spectral_similarity(user_y, user_sr, app_y, app_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "872c1c06-7f13-4d1e-99d8-0498a76d8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intra_song_similarity(audio_file: str, similarity_func: Callable=aggregate_spectral_similarity) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity between the first two-thirds and the last two-thirds of an audio file.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_file (str): Path to the audio file.\n",
    "    - sr (int): Sampling rate to use when loading the audio file.\n",
    "    - similarity_func (function): A function that computes similarity between two audio segments.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The computed similarity value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "    \n",
    "    # Split the audio into two segments: first two-thirds and last two-thirds\n",
    "    two_thirds_length = int(2 * len(y) / 3)\n",
    "    segment1 = y[:two_thirds_length]\n",
    "    segment2 = y[len(y) - two_thirds_length:]\n",
    "    \n",
    "    # Compute the similarity between the two segments\n",
    "    similarity = similarity_func(segment1, sr, segment2, sr)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1f2ac38d-cde7-46d3-a695-ced564bac154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/fma_small/000/000002.mp3'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fa6e451c-7af7-45dd-842e-be5ab370b2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_spec =  (1025, 1722)\n",
      "user_spec =  (1025, 1723)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9542035314611892, 0.9600788605688815)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_intra_song_similarity(filename1), compute_intra_song_similarity(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c175a-9ef6-4539-80d0-eac41188e4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476df7ae-0ee6-49a5-a479-ee10caf43c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627d503-544b-4948-b6dc-9ba6ba4a7329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (audio)",
   "language": "python",
   "name": "audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
